# AR-SmartAssistant Configuration
# Copy this to config.yaml and adjust for your setup

storage:
  root: "./data"
  audio_segments: "./data/audio_segments"
  databases:
    brain_main: "./data/brain_main.db"
    system_metrics: "./data/system_metrics.db"

audio:
  # Audio input source: "microphone" for local mic, "websocket" for Glass
  input_source: "microphone"

  capture:
    sample_rate_hz: 16000
    encoding: "PCM_16BIT"
    channel: "MONO"
    source: "VOICE_RECOGNITION"
    buffer_size_bytes: 3200   # 200ms chunks at 16kHz
    device_index: null  # null = default device, or specify device index

  preprocessing:
    noise_suppressor:
      enabled: true
    automatic_gain_control:
      enabled: true
    acoustic_echo_canceler:
      enabled: true

  vad:
    type: "energy_based"
    energy_threshold_db: -45
    frame_duration_ms: 30
    min_speech_duration_ms: 300
    padding_duration_ms: 300

  asr:
    model: "faster-whisper"
    model_size: "small.en"
    device: "cuda"  # "cuda" or "cpu"
    compute_type: "int8"  # int8, float16, float32
    beam_size: 5
    language: "en"
    confidence_threshold: 0.7
    vad_filter: true

  speaker_id:
    model: "resemblyzer"
    embedding_dim: 256
    similarity_metric: "cosine"
    self_match_threshold: 0.80
    unknown_threshold: 0.65
    enrollment:
      required_phrases: 5
      min_duration_per_phrase_sec: 6.0
      max_embedding_std_dev: 0.15

websocket:
  # For Glass integration
  enabled: false
  host: "0.0.0.0"
  port: 8765

llm:
  provider: "ollama"  # "ollama" or "openai"
  model: "llama3.1:8b"
  temperature: 0.3
  max_tokens: 1000
  base_url: "http://localhost:11434"  # Ollama default

embeddings:
  provider: "chromadb"
  model: "all-MiniLM-L6-v2"
  collection_name: "memories"
  persist_directory: "./data/chroma"

debug_ui:
  enabled: true
  host: "127.0.0.1"
  port: 5000
  auto_open_browser: true

logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "./data/logs/app.log"
  max_bytes: 10485760  # 10MB
  backup_count: 5
